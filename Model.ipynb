{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5olyMpkBsmXEzyp7AOhlT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ced49663aad4862ad49f765e0c0c9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Input:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7383245d10fc433e8f219e75df1563f9",
            "placeholder": "Type something (e.g., \"the cat is black\")",
            "style": "IPY_MODEL_7ff2b5c8db664787a28179a43707b6de",
            "value": "my name is ayushman lohani"
          }
        },
        "7383245d10fc433e8f219e75df1563f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ff2b5c8db664787a28179a43707b6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e55dc26d9ad4464499bcff588afccf12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b72cdfdde7ef4327aa86f57c7f6bf4cd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fa610d14ccd441909327892d0c787d68",
            "value": "French: mon nom est intol r ."
          }
        },
        "b72cdfdde7ef4327aa86f57c7f6bf4cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa610d14ccd441909327892d0c787d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmanlohani/Neural-translator-eng-fr-/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Tokenizer Functions ---\n",
        "def basic_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'([.,!?;])', r' \\1 ', text)\n",
        "    text = re.sub(r'([\"\\'])', r' \\1 ', text)\n",
        "    text = re.sub(r'[^a-z0-9.,!?;\\'\\\" ]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text.split()\n",
        "\n",
        "def create_vocabulary(tokens_list, min_frequency=2):\n",
        "    token_counts = defaultdict(int)\n",
        "    for tokens in tokens_list:\n",
        "        for token in tokens:\n",
        "            token_counts[token] += 1\n",
        "    vocab = {\n",
        "        '<pad>': 0,\n",
        "        '<unk>': 1,\n",
        "        '<sos>': 2,\n",
        "        '<eos>': 3\n",
        "    }\n",
        "\n",
        "    token_idx = len(vocab)\n",
        "    for token, count in token_counts.items():\n",
        "        if count >= min_frequency:\n",
        "            vocab[token] = token_idx\n",
        "            token_idx += 1\n",
        "    return vocab\n",
        "\n",
        "# --- Data Loading and Vocab Building ---\n",
        "def load_and_process_data(path):\n",
        "    eng_sentences = []\n",
        "    fr_sentences = []\n",
        "    eng_tokens_list = []\n",
        "    fr_tokens_list = []\n",
        "\n",
        "    print(\"Loading CSV and tokenizing...\")\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)  # Skip header\n",
        "        for row in csv_reader:\n",
        "            if len(row) >= 2:\n",
        "                # Store raw text\n",
        "                eng_sentences.append(row[0])\n",
        "                fr_sentences.append(row[1])\n",
        "\n",
        "                # Store tokens for vocab building\n",
        "                eng_tokens_list.append(basic_tokenize(row[0]))\n",
        "                fr_tokens_list.append(basic_tokenize(row[1]))\n",
        "\n",
        "    print(f\"Loaded {len(eng_sentences)} pairs.\")\n",
        "\n",
        "    # Build Vocabularies in-memory\n",
        "    print(\"Building English Vocabulary...\")\n",
        "    eng_vocab = create_vocabulary(eng_tokens_list, min_frequency=2)\n",
        "    print(\"Building French Vocabulary...\")\n",
        "    fr_vocab = create_vocabulary(fr_tokens_list, min_frequency=2)\n",
        "\n",
        "    print(f\"English Vocab Size: {len(eng_vocab)}\")\n",
        "    print(f\"French Vocab Size: {len(fr_vocab)}\")\n",
        "\n",
        "    return eng_sentences, fr_sentences, eng_vocab, fr_vocab\n",
        "\n",
        "# Execute Loading\n",
        "try:\n",
        "    eng_sentences, fr_sentences, eng_vocab, fr_vocab = load_and_process_data('eng_french.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'eng_french.csv' not found. Please upload the file to Colab.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ySMv2_DUyrz",
        "outputId": "c1005ac6-0a59-46da-dcde-fa0a0d6573fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading CSV and tokenizing...\n",
            "Loaded 175621 pairs.\n",
            "Building English Vocabulary...\n",
            "Building French Vocabulary...\n",
            "English Vocab Size: 9782\n",
            "French Vocab Size: 13478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# --- Function for Tokenization ---\n",
        "def tokenize_and_pad(text, vocab, max_length=128):\n",
        "    \"\"\"\n",
        "    Converts text to list of indices, adds SOS/EOS, and pads to max_length.\n",
        "\n",
        "    \"\"\"\n",
        "    # Tokenize using the same basic_tokenize function\n",
        "    tokens = basic_tokenize(text)\n",
        "\n",
        "    encoded = [vocab['<sos>']] + \\\n",
        "              [vocab.get(token, vocab['<unk>']) for token in tokens] + \\\n",
        "              [vocab['<eos>']]\n",
        "\n",
        "    # Truncate if too long (account for SOS/EOS)\n",
        "    if len(encoded) > max_length:\n",
        "        encoded = encoded[:max_length]\n",
        "\n",
        "    # Pad with zeros (vocab['<pad>'] is 0)\n",
        "    padding = [vocab['<pad>']] * (max_length - len(encoded))\n",
        "    encoded = encoded + padding\n",
        "\n",
        "    return torch.tensor(encoded, dtype=torch.long)\n",
        "\n",
        "# --- Dataset Class ---\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_length=128):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.src_sentences[idx]\n",
        "        tgt_text = self.tgt_sentences[idx]\n",
        "\n",
        "        src_ids = tokenize_and_pad(src_text, self.src_vocab, self.max_length)\n",
        "        tgt_ids = tokenize_and_pad(tgt_text, self.tgt_vocab, self.max_length)\n",
        "\n",
        "        return {\n",
        "            'src_ids': src_ids,\n",
        "            'tgt_ids': tgt_ids,\n",
        "            'src_text': src_text,\n",
        "            'tgt_text': tgt_text\n",
        "        }\n",
        "\n",
        "# --- Split Data and Create Loaders ---\n",
        "\n",
        "# 1. Shuffle indices\n",
        "indices = list(range(len(eng_sentences)))\n",
        "np.random.seed(42) # Fixed seed for reproducibility\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# 2. Reorder lists\n",
        "eng_sentences_shuffled = [eng_sentences[i] for i in indices]\n",
        "fr_sentences_shuffled = [fr_sentences[i] for i in indices]\n",
        "\n",
        "# 3. 90/10 Split\n",
        "split_idx = int(len(eng_sentences) * 0.9)\n",
        "\n",
        "train_eng = eng_sentences_shuffled[:split_idx]\n",
        "train_fr = fr_sentences_shuffled[:split_idx]\n",
        "val_eng = eng_sentences_shuffled[split_idx:]\n",
        "val_fr = fr_sentences_shuffled[split_idx:]\n",
        "\n",
        "print(f\"Training samples: {len(train_eng)}\")\n",
        "print(f\"Validation samples: {len(val_eng)}\")\n",
        "\n",
        "# 4. Create Datasets\n",
        "train_dataset = TranslationDataset(train_eng, train_fr, eng_vocab, fr_vocab)\n",
        "val_dataset = TranslationDataset(val_eng, val_fr, eng_vocab, fr_vocab)\n",
        "\n",
        "# 5. Create DataLoaders\n",
        "batch_size = 64 # Reduced slightly to ensure stability\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Verify a single batch\n",
        "sample_batch = next(iter(train_dataloader))\n",
        "print(f\"Batch shape src: {sample_batch['src_ids'].shape}\")\n",
        "print(f\"Batch shape tgt: {sample_batch['tgt_ids'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37jYCefKUypK",
        "outputId": "a5da52f5-296d-4aeb-cb87-a58afb4136fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 158058\n",
            "Validation samples: 17563\n",
            "Batch shape src: torch.Size([64, 128])\n",
            "Batch shape tgt: torch.Size([64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# --- Configuration Class ---\n",
        "class TransformerConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        block_size=128,      # Matches dataset max_length\n",
        "        n_layer=6,           # Encoder layers\n",
        "        n_pre_cross_layer=3, # Decoder layers before cross-attention\n",
        "        n_cross_layer=3,     # Decoder layers with cross-attention\n",
        "        n_embd=256,\n",
        "        num_heads=8,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_pre_cross_layer = n_pre_cross_layer\n",
        "        self.n_cross_layer = n_cross_layer\n",
        "        self.n_embd = n_embd\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "\n",
        "# --- Sub-components ---\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    \"\"\" Standard Feed Forward Layer \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.num_heads == 0\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_size = config.n_embd // config.num_heads\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.q_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.k_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.v_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.out_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, q, k=None, v=None, mask=None, is_causal=False):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # If k, v are None, this is self-attention (use q)\n",
        "        if k is None: k = q\n",
        "        if v is None: v = q\n",
        "\n",
        "        q_out = self.q_proj(q)\n",
        "        k_out = self.k_proj(k)\n",
        "        v_out = self.v_proj(v)\n",
        "\n",
        "        # Reshape for multi-head\n",
        "        # (B, T, num_heads, head_size) -> (B, num_heads, T, head_size)\n",
        "        q_out = q_out.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        k_out = k_out.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        v_out = v_out.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = (q_out @ k_out.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "\n",
        "        if is_causal:\n",
        "            seq_len = q_out.size(-2)\n",
        "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=q.device), diagonal=1)\n",
        "            scores.masked_fill_(causal_mask, float('-inf'))\n",
        "\n",
        "        if mask is not None:\n",
        "            # Mask shape handling\n",
        "            if mask.dim() == 3: mask = mask.unsqueeze(1) # Add head dimension\n",
        "            scores.masked_fill_(~mask, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn @ v_out\n",
        "\n",
        "        # Restore shape\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.n_embd)\n",
        "        out = self.out_proj(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, mask=None, is_causal=False):\n",
        "        x = x + self.attn(self.ln1(x), mask=mask, is_causal=is_causal)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.self_attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.cross_attn = MultiHeadAttention(config)\n",
        "        self.ln3 = nn.LayerNorm(config.n_embd)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, enc_out, self_mask=None, cross_mask=None):\n",
        "        # Self attention (Causal)\n",
        "        x = x + self.self_attn(self.ln1(x), mask=self_mask, is_causal=True)\n",
        "        # Cross attention\n",
        "        x = x + self.cross_attn(q=self.ln2(x), k=enc_out, v=enc_out, mask=cross_mask)\n",
        "        # Feed Forward\n",
        "        x = x + self.ffwd(self.ln3(x))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask=mask)\n",
        "        return self.ln_f(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.pre_blocks = nn.ModuleList([Block(config) for _ in range(config.n_pre_cross_layer)])\n",
        "        self.cross_blocks = nn.ModuleList([CrossAttentionBlock(config) for _ in range(config.n_cross_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x, enc_out, padding_mask=None, cross_mask=None):\n",
        "        # Pre-cross blocks (causal only)\n",
        "        for block in self.pre_blocks:\n",
        "            x = block(x, mask=padding_mask, is_causal=True)\n",
        "\n",
        "        # Cross-attention blocks\n",
        "        for block in self.cross_blocks:\n",
        "            x = block(x, enc_out, self_mask=padding_mask, cross_mask=cross_mask)\n",
        "\n",
        "        return self.ln_f(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.src_tok_emb = nn.Embedding(config.src_vocab_size, config.n_embd)\n",
        "        self.tgt_tok_emb = nn.Embedding(config.tgt_vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "        self.head = nn.Linear(config.n_embd, config.tgt_vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, src_ids, tgt_ids, src_mask=None, tgt_mask=None):\n",
        "        B, T_src = src_ids.size()\n",
        "        _, T_tgt = tgt_ids.size()\n",
        "\n",
        "        # Source Embeddings\n",
        "        src_emb = self.src_tok_emb(src_ids)\n",
        "        src_pos = self.pos_emb[:, :T_src, :]\n",
        "        x = self.drop(src_emb + src_pos)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_out = self.encoder(x, src_mask)\n",
        "\n",
        "        # Target Embeddings\n",
        "        tgt_emb = self.tgt_tok_emb(tgt_ids)\n",
        "        tgt_pos = self.pos_emb[:, :T_tgt, :]\n",
        "        y = self.drop(tgt_emb + tgt_pos)\n",
        "\n",
        "        # Decoder\n",
        "        y = self.decoder(y, encoder_out, padding_mask=tgt_mask, cross_mask=src_mask)\n",
        "\n",
        "        # Head\n",
        "        logits = self.head(y)\n",
        "        return logits\n",
        "\n",
        "print(\"Transformer Architecture defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71g5kddaUynL",
        "outputId": "30ba6d6f-20fc-4460-8f43-1cd11cbe18b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Architecture defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "import time\n",
        "\n",
        "# --- Initialize Model ---\n",
        "config = TransformerConfig(\n",
        "    src_vocab_size=len(eng_vocab),\n",
        "    tgt_vocab_size=len(fr_vocab),\n",
        "    block_size=128,\n",
        "    n_layer=6,\n",
        "    n_pre_cross_layer=3,\n",
        "    n_cross_layer=3,\n",
        "    n_embd=256,\n",
        "    num_heads=8,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "model = Transformer(config).to(device)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=3e-4):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Scheduler: reduce LR if validation loss stops improving\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignore <pad> tokens\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # --- Training Phase ---\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=False)\n",
        "        for batch in progress_bar:\n",
        "            src_ids = batch['src_ids'].to(device)\n",
        "            tgt_ids = batch['tgt_ids'].to(device)\n",
        "\n",
        "            # Create padding masks (1 for tokens, 0 for pad)\n",
        "            # Shape: [batch, 1, 1, seq_len]\n",
        "            src_mask = (src_ids != 0).unsqueeze(1).unsqueeze(2)\n",
        "            tgt_mask = (tgt_ids != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "            # Forward pass\n",
        "            # Input to decoder is tgt_ids without the last token\n",
        "            decoder_input = tgt_ids[:, :-1]\n",
        "            decoder_mask = tgt_mask[:, :, :, :-1]\n",
        "\n",
        "            logits = model(src_ids, decoder_input, src_mask, decoder_mask)\n",
        "\n",
        "            # Calculate loss\n",
        "            # Target is tgt_ids without the first token (<sos>)\n",
        "            targets = tgt_ids[:, 1:].contiguous().view(-1)\n",
        "            predictions = logits.contiguous().view(-1, logits.size(-1))\n",
        "\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', leave=False):\n",
        "                src_ids = batch['src_ids'].to(device)\n",
        "                tgt_ids = batch['tgt_ids'].to(device)\n",
        "\n",
        "                src_mask = (src_ids != 0).unsqueeze(1).unsqueeze(2)\n",
        "                tgt_mask = (tgt_ids != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "                decoder_input = tgt_ids[:, :-1]\n",
        "                decoder_mask = tgt_mask[:, :, :, :-1]\n",
        "\n",
        "                logits = model(src_ids, decoder_input, src_mask, decoder_mask)\n",
        "\n",
        "                targets = tgt_ids[:, 1:].contiguous().view(-1)\n",
        "                predictions = logits.contiguous().view(-1, logits.size(-1))\n",
        "\n",
        "                loss = criterion(predictions, targets)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Step scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1} | Time: {elapsed:.0f}s | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print(f\"--> Best model saved (Loss: {best_val_loss:.4f})\")\n",
        "\n",
        "# --- Run Training ---\n",
        "train_model(model, train_dataloader, val_dataloader, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCT9_PSzUykz",
        "outputId": "f58b3f3c-c73a-44b9-d17e-1f7cff41e1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Time: 774s | Train Loss: 2.6536 | Val Loss: 1.6682\n",
            "--> Best model saved (Loss: 1.6682)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Time: 771s | Train Loss: 1.4024 | Val Loss: 1.2152\n",
            "--> Best model saved (Loss: 1.2152)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Time: 772s | Train Loss: 1.0573 | Val Loss: 1.0435\n",
            "--> Best model saved (Loss: 1.0435)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Time: 772s | Train Loss: 0.8795 | Val Loss: 0.9510\n",
            "--> Best model saved (Loss: 0.9510)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Time: 771s | Train Loss: 0.7638 | Val Loss: 0.9047\n",
            "--> Best model saved (Loss: 0.9047)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Time: 772s | Train Loss: 0.6801 | Val Loss: 0.8736\n",
            "--> Best model saved (Loss: 0.8736)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Time: 772s | Train Loss: 0.6168 | Val Loss: 0.8553\n",
            "--> Best model saved (Loss: 0.8553)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Time: 772s | Train Loss: 0.5648 | Val Loss: 0.8410\n",
            "--> Best model saved (Loss: 0.8410)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Time: 772s | Train Loss: 0.5220 | Val Loss: 0.8412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Time: 772s | Train Loss: 0.4860 | Val Loss: 0.8402\n",
            "--> Best model saved (Loss: 0.8402)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Checking for saved model...\")\n",
        "if os.path.exists('best_model.pt'):\n",
        "    print(f\"File found! Size: {os.path.getsize('best_model.pt') / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    # --- Download to Local Computer ---\n",
        "\n",
        "    files.download('best_model.pt')\n",
        "\n",
        "    # --- Save to Google Drive ---\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # This saves it to the main folder of my Drive\n",
        "    destination_path = '/content/drive/MyDrive/french_translator_model.pt'\n",
        "\n",
        "    print(f\"Copying model to {destination_path}...\")\n",
        "    shutil.copy('best_model.pt', destination_path)\n",
        "\n",
        "    # Saving vocabularies as well\n",
        "    # Saved them as text files for simplicity\n",
        "    def save_vocab(vocab, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            for token, idx in vocab.items():\n",
        "                f.write(f\"{token}\\t{idx}\\n\")\n",
        "\n",
        "    save_vocab(eng_vocab, 'eng_vocab.txt')\n",
        "    save_vocab(fr_vocab, 'fr_vocab.txt')\n",
        "\n",
        "    shutil.copy('eng_vocab.txt', '/content/drive/MyDrive/eng_vocab.txt')\n",
        "    shutil.copy('fr_vocab.txt', '/content/drive/MyDrive/fr_vocab.txt')\n",
        "\n",
        "    print(\"SUCCESS: Model and vocabularies saved to Google Drive.\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: 'best_model.pt' not found. Did the training loop finish at least one validation phase?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "CSBHOZD1Uyic",
        "outputId": "94419192-bcff-41c7-9d07-fa8b44e8f97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for saved model...\n",
            "File found! Size: 75.26 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_15debdea-5cc6-4dde-b57a-da97677c8f46\", \"best_model.pt\", 78914807)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Copying model to /content/drive/MyDrive/french_translator_model.pt...\n",
            "SUCCESS: Model and vocabularies saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Best Model Weights ---\n",
        "print(\"Loading best model weights...\")\n",
        "try:\n",
        "    checkpoint = torch.load('best_model.pt', map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    print(\"Best model loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: 'best_model.pt' not found. Using current model weights.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# --- Translation Function ---\n",
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, device, max_length=50):\n",
        "    # 1. Tokenize and Prepare Source\n",
        "    tokens = basic_tokenize(sentence)\n",
        "    src_indices = [src_vocab['<sos>']] + \\\n",
        "                  [src_vocab.get(t, src_vocab['<unk>']) for t in tokens] + \\\n",
        "                  [src_vocab['<eos>']]\n",
        "\n",
        "    # Padding is strictly not necessary for batch_size=1, but good for consistency\n",
        "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device) # [1, seq_len]\n",
        "    src_mask = (src_tensor != 0).unsqueeze(1).unsqueeze(2) # [1, 1, 1, seq_len]\n",
        "\n",
        "    # 2. Encode\n",
        "    with torch.no_grad():\n",
        "        src_emb = model.src_tok_emb(src_tensor)\n",
        "        src_pos = model.pos_emb[:, :src_tensor.size(1), :]\n",
        "        encoder_out = model.encoder(model.drop(src_emb + src_pos), src_mask)\n",
        "\n",
        "    # 3. Decode (Autoregressive)\n",
        "    tgt_indices = [tgt_vocab['<sos>']]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n",
        "        tgt_mask = (tgt_tensor != 0).unsqueeze(1).unsqueeze(2)\n",
        "        # Causal mask for decoder is handled inside the model's forward/blocks,\n",
        "        # but need to ensure the mask passed covers the current sequence length.\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get embeddings for current target sequence\n",
        "            tgt_emb = model.tgt_tok_emb(tgt_tensor)\n",
        "            tgt_pos = model.pos_emb[:, :tgt_tensor.size(1), :]\n",
        "            y = model.drop(tgt_emb + tgt_pos)\n",
        "\n",
        "            # Pass through Decoder\n",
        "            # Note: We create a causal mask implicit in the Decoder logic,\n",
        "            # but we pass the padding mask (tgt_mask) and cross_mask (src_mask)\n",
        "            output = model.decoder(y, encoder_out, padding_mask=tgt_mask, cross_mask=src_mask)\n",
        "\n",
        "            # Project to vocab\n",
        "            logits = model.head(output)\n",
        "\n",
        "            # Get last token logits\n",
        "            next_token_logits = logits[0, -1, :]\n",
        "            next_token_id = next_token_logits.argmax().item()\n",
        "\n",
        "            tgt_indices.append(next_token_id)\n",
        "\n",
        "            # Stop if End of Sequence\n",
        "            if next_token_id == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "    # 4. Convert Indices to Text\n",
        "    # Create reverse vocabulary mapping\n",
        "    idx_to_word = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "    translated_tokens = []\n",
        "    for idx in tgt_indices:\n",
        "        token = idx_to_word.get(idx, '')\n",
        "        if token not in ['<sos>', '<eos>', '<pad>']:\n",
        "            translated_tokens.append(token)\n",
        "\n",
        "    return \" \".join(translated_tokens)\n",
        "\n",
        "# --- Test on Manual Inputs ---\n",
        "print(\"\\n--- Testing Translation ---\")\n",
        "test_sentences = [\n",
        "    \"hello how are you\",\n",
        "    \"i love programming\",\n",
        "    \"the cat is on the table\",\n",
        "    \"my keyboard is black and it is of gaming type\"\n",
        "]\n",
        "\n",
        "for s in test_sentences:\n",
        "    trans = translate_sentence(model, s, eng_vocab, fr_vocab, device)\n",
        "    print(f\"En: {s}\")\n",
        "    print(f\"Fr: {trans}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5qRDGmqUyf7",
        "outputId": "437320b8-ed31-482a-872a-e7e92dc2ea97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model weights...\n",
            "Best model loaded successfully.\n",
            "\n",
            "--- Testing Translation ---\n",
            "En: hello how are you\n",
            "Fr: salut tes vous .\n",
            "--------------------\n",
            "En: i love programming\n",
            "Fr: j ' adore les barbecues .\n",
            "--------------------\n",
            "En: the cat is on the table\n",
            "Fr: le chat est sur la table .\n",
            "--------------------\n",
            "En: my keyboard is black and it is of gaming type\n",
            "Fr: mon clavier est noir et c ' est du genre de type de <unk> .\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import random\n",
        "\n",
        "def calculate_bleu(data_loader, model, src_vocab, tgt_vocab, device, num_samples=100):\n",
        "    model.eval()\n",
        "    sources = []\n",
        "    targets = [] # References need to be a list of lists\n",
        "    hypotheses = []\n",
        "\n",
        "    # Get all validation data\n",
        "    print(f\"Preparing to calculate BLEU on {num_samples} samples...\")\n",
        "\n",
        "    # Extract random indices\n",
        "    indices = random.sample(range(len(data_loader.dataset)), min(num_samples, len(data_loader.dataset)))\n",
        "\n",
        "    # Reverse vocab for decoding\n",
        "    idx_to_word = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(indices, desc=\"Translating\"):\n",
        "            item = data_loader.dataset[i]\n",
        "            src_text = item['src_text']\n",
        "            tgt_text = item['tgt_text']\n",
        "\n",
        "            # Translate\n",
        "            prediction = translate_sentence(model, src_text, src_vocab, tgt_vocab, device)\n",
        "\n",
        "            # Tokenize for BLEU (simple split)\n",
        "            ref_tokens = tgt_text.split()\n",
        "            pred_tokens = prediction.split()\n",
        "\n",
        "            targets.append([ref_tokens]) # List of references (we have 1 per sentence)\n",
        "            hypotheses.append(pred_tokens)\n",
        "\n",
        "            # Print first 3 examples to inspect visually\n",
        "            if len(targets) <= 3:\n",
        "                print(f\"\\nRef: {tgt_text}\")\n",
        "                print(f\"Pred: {prediction}\")\n",
        "\n",
        "    # Calculate BLEU-4\n",
        "    score = corpus_bleu(targets, hypotheses) * 100\n",
        "    return score\n",
        "\n",
        "# Run Evaluation\n",
        "bleu_score = calculate_bleu(val_dataloader, model, eng_vocab, fr_vocab, device)\n",
        "print(f\"\\nBLEU Score: {bleu_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Xi7rcxUydq",
        "outputId": "7f6f502f-d245-4c00-c528-53794f273eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing to calculate BLEU on 100 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:   3%|â–Ž         | 3/100 [00:00<00:04, 21.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ref: Elle l'a battu Ã  mort.\n",
            "Pred: elle l ' a battu en d tail .\n",
            "\n",
            "Ref: Elle vit Ã  Kyoto.\n",
            "Pred: elle habite kyoto .\n",
            "\n",
            "Ref: Tom voulait que Mary dise oui.\n",
            "Pred: tom voulait que mary dise oui .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 16.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU Score: 9.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recalculate BLEU with Normalization ---\n",
        "def calculate_bleu_normalized(data_loader, model, src_vocab, tgt_vocab, device, num_samples=100):\n",
        "    model.eval()\n",
        "    targets = []\n",
        "    hypotheses = []\n",
        "\n",
        "    # Use same seed for consistency with previous run if desired,\n",
        "    # but here we just sample again.\n",
        "    indices = random.sample(range(len(data_loader.dataset)), min(num_samples, len(data_loader.dataset)))\n",
        "\n",
        "    idx_to_word = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "    print(f\"Recalculating BLEU (Normalized) on {len(indices)} samples...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(indices, desc=\"Translating\"):\n",
        "            item = data_loader.dataset[i]\n",
        "            src_text = item['src_text']\n",
        "            tgt_text = item['tgt_text'] # Contains \"Tom\"\n",
        "\n",
        "            # Translate\n",
        "            prediction = translate_sentence(model, src_text, src_vocab, tgt_vocab, device)\n",
        "\n",
        "            # NORMALIZE: Convert both to lowercase before splitting\n",
        "            ref_tokens = tgt_text.lower().split()\n",
        "            pred_tokens = prediction.lower().split()\n",
        "\n",
        "            targets.append([ref_tokens])\n",
        "            hypotheses.append(pred_tokens)\n",
        "\n",
        "    score = corpus_bleu(targets, hypotheses) * 100\n",
        "    return score\n",
        "\n",
        "# Run Corrected Evaluation\n",
        "true_bleu_score = calculate_bleu_normalized(val_dataloader, model, eng_vocab, fr_vocab, device)\n",
        "print(f\"\\nCorrected BLEU Score: {true_bleu_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwr61uByUybK",
        "outputId": "299b96c8-f694-4a8d-b6d8-533194dd4697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recalculating BLEU (Normalized) on 100 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:06<00:00, 14.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corrected BLEU Score: 15.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"--- English to French Neural Translator ---\")\n",
        "print(\"Enter an English sentence to translate:\")\n",
        "\n",
        "# Create widgets\n",
        "text_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type something (e.g., \"the cat is black\")',\n",
        "    description='Input:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "output_label = widgets.Label(value=\"Translation will appear here...\")\n",
        "\n",
        "def on_submit(change):\n",
        "    if change.new:\n",
        "        sentence = change.new\n",
        "        # Translate\n",
        "        try:\n",
        "            translation = translate_sentence(model, sentence, eng_vocab, fr_vocab, device)\n",
        "            output_label.value = f\"French: {translation}\"\n",
        "        except Exception as e:\n",
        "            output_label.value = f\"Error: {str(e)}\"\n",
        "\n",
        "text_input.observe(on_submit, names='value')\n",
        "\n",
        "display(text_input, output_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "8ced49663aad4862ad49f765e0c0c9a1",
            "7383245d10fc433e8f219e75df1563f9",
            "7ff2b5c8db664787a28179a43707b6de",
            "e55dc26d9ad4464499bcff588afccf12",
            "b72cdfdde7ef4327aa86f57c7f6bf4cd",
            "fa610d14ccd441909327892d0c787d68"
          ]
        },
        "id": "dh-zpWy82XPr",
        "outputId": "c200ab1c-12af-40de-c9ad-e1a80f349f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- English to French Neural Translator ---\n",
            "Enter an English sentence to translate:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Input:', placeholder='Type something (e.g., \"the cat is black\")')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ced49663aad4862ad49f765e0c0c9a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Label(value='Translation will appear here...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e55dc26d9ad4464499bcff588afccf12"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZPZChtnUyYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "223uPCaoUyWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RGD8yGuoUyTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6tuJxEhuUyRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r3XHCxgQUyO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFO-qe6UUyMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPuNUBV1UyKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZm9EAH8UyHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZdJQ8IdUyFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9H0wGY45UyC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4c6JxHkcUyAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AOXRosOAUx-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6Nx9NhFUx70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0T21iP4Ux5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0uIr5474Ux3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xPtBZAiUx06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ld_c9DsUxyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xtpaa4ZTUxwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDqdN-USUxty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RoMZoqZFUxri"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}